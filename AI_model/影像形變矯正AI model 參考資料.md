1. Palette: Image-to-Image Diffusion Models
	1. [論文連結](https://arxiv.org/pdf/2111.05826.pdf)
	2. 描述 : 這篇論文發展了一個統一的框架，用於基於條件擴散模型的影像轉換，並在四個具有挑戰性的影像轉換任務上進行評估，包括上色、修補、取消裁剪和JPEG修復。我們簡單的影像轉換擴散模型在所有任務上均優於強大的GAN和回歸基線，而無需特定任務的超參數調整、架構自定義，或任何輔助損失或複雜的新技術。我們發現在去雜訊擴散目標中的L2與L1損失對樣本多樣性的影響，並通過實驗研究展示了神經架構中自注意力的重要性。重要的是我們提倡基於ImageNet的統一評估協議，包括人工評估和樣本品質分數（FID、Inception Score、預訓練ResNet-50的分類準確度，以及與原始圖像的感知距離）。我們期望這種標準化的評估協議能推動影像轉換研究的進步。最後我們展示了一個通用的多任務擴散模型與特定任務訓練的模型相比表現同樣好甚至更好。
2. RePaint: Inpainting using Denoising Diffusion Probabilistic Models
	1. [論文連結](https://arxiv.org/pdf/2201.09865.pdf)
	2. 描述 : 自由形式修補是在由任意二元遮罩指定的區域向圖像添加新內容的任務。大多數現有方法訓練於特定遮罩分佈，這限制了它們對未見遮罩類型的泛化能力。此外使用像素級和感知損失進行訓練通常會導致對缺失區域進行簡單的紋理擴展，而非具有語義意義的生成。所以在這項工作中，我們提出了 RePaint：基於去雜訊擴散機率模型（DDPM）的修補方法，適用於極端遮罩。我們使用預訓練的無條件 DDPM 作為生成先驗。為了條件化生成過程，我們僅通過使用給定圖像訊息對未遮罩區域進行反向擴散迭代的抽樣。由於這種技術不修改或條件化原始 DDPM 網絡本身，模型能夠針對任何修補形式生成高品質且多樣化的輸出圖像。最後我們驗證了我們的方法在臉部和通用圖像修補上的效果，包括使用標準和極端遮罩。RePaint 在六種遮罩分佈中至少有五種的表現優於最先進的自回歸和GAN方法。
3. Denoising Diffusion Restoration Models
	1. [論文連結](https://arxiv.org/pdf/2201.11793.pdf)
	2. 描述 : 這篇論文討論了圖像修復中的線性反問題，提出了一種新的方法：Denoising Diffusion Restoration Models（DDRM）。這種方法利用預先訓練的去雜訊擴散生成模型，能有效地解決各種圖像修復任務，如超解析度、去模糊、修補和上色。相較於其他無監督方法，DDRM在圖像重建品質、感知品質和執行時間方面表現更優越，在ImageNet數據集上比競爭者快了5倍。同時DDRM在觀測訓練集以外的自然圖像中也表現出很好的泛化能力
4. Towards Robust Blind Face Restoration with Codebook Lookup Transformer
	1. [論文連結](https://arxiv.org/pdf/2206.11253.pdf)
	2. Blind face restoration is a highly ill-posed problem that often requires auxiliary guidance to 1) improve the mapping from degraded inputs to desired outputs, or 2) complement high-quality details lost in the inputs. In this paper, we demonstrate that a learned discrete codebook prior in a small proxy space largely reduces the uncertainty and ambiguity of restoration mapping by casting blind face restoration as a code prediction task, while providing rich visual atoms for generating highquality faces. Under this paradigm, we propose a Transformer-based prediction network, named CodeFormer, to model the global composition and context of the low-quality faces for code prediction, enabling the discovery of natural faces that closely approximate the target faces even when the inputs are severely degraded. To enhance the adaptiveness for different degradation, we also propose a controllable feature transformation module that allows a flexible trade-off between fidelity and quality. Thanks to the expressive codebook prior and global modeling, CodeFormer outperforms the state of the arts in both quality and fidelity, showing superior robustness to degradation. Extensive experimental results on synthetic an
5. Towards Real-World Blind Face Restoration with Generative Facial Prior
	1. [論文連結](https://arxiv.org/pdf/2101.04061.pdf)
	2. Blind face restoration usually relies on facial priors, such as facial geometry prior or reference prior, to restore realistic and faithful details. However, very low-quality inputs cannot offer accurate geometric prior while highquality references are inaccessible, limiting the applicability in real-world scenarios. In this work, we propose GFP-GAN that leverages rich and diverse priors encapsulated in a pretrained face GAN for blind face restoration. This Generative Facial Prior (GFP) is incorporated into the face restoration process via spatial feature transform layers, which allow our method to achieve a good balance of realness and fidelity. Thanks to the powerful generative facial prior and delicate designs, our GFP-GAN could jointly restore facial details and enhance colors with just a single forward pass, while GAN inversion methods require image-specific optimization at inference. Extensive experiments show that our method achieves superior performance to prior art on both synthetic and real-world datasets.
6. 